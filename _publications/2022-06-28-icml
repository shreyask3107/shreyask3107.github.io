---
title: "Balancing discriminability and transferability for source-free domain adaptation"
collection: publications
permalink: /publication/icml
date: 2022-06-28
venue: 'International Conference on Machine Learning'
---
Authors: Jogendra Nath Kundu, Akshay R Kulkarni, Suvaansh Bhambri, Deepesh Mehta, **Shreyas Anand Kulkarni**, Varun Jampani, Venkatesh Babu Radhakrishnan

[[PMLR Landing Page]](https://proceedings.mlr.press/v162/kundu22a.html)[[Project Website]](https://sites.google.com/view/mixup-sfda)
Conventional domain adaptation (DA) techniques aim to improve domain transferability by learning domain-invariant representations; while concurrently preserving the task-discriminability knowledge gathered from the labeled source data. However, the requirement of simultaneous access to labeled source and unlabeled target renders them unsuitable for the challenging source-free DA setting. The trivial solution of realizing an effective original to generic domain mapping improves transferability but degrades task discriminability. Upon analyzing the hurdles from both theoretical and empirical standpoints, we derive novel insights to show that a mixup between original and corresponding translated generic samples enhances the discriminability-transferability trade-off while duly respecting the privacy-oriented source-free setting. A simple but effective realization of the proposed insights on top of the existing source-free DA approaches yields state-of-the-art performance with faster convergence. Beyond single-source, we also outperform multi-source prior-arts across both classification and semantic segmentation benchmarks. 
